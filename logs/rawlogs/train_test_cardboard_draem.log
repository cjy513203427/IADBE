/bin/bash /home/jinyao/PycharmProjects/IADBE/train_test_cardboard_draem.sh
(IADBE) jinyao@jinyao-System-Product-Name:~/PycharmProjects/IADBE$ /bin/bash /home/jinyao/PycharmProjects/IADBE/train_test_cardboard_draem.sh
Running command: anomalib train --data ../configs/data/custom_dataset_normal_abnormal_cardboard.yaml --model anomalib.models.Draem
2024-08-05 15:30:48,965 - anomalib.utils.config - WARNING - Anomalib currently does not support multi-gpu training. Setting devices to 1.
[08/05/24 15:30:48] WARNING  Anomalib currently does not support multi-gpu training. Setting devices to 1.                                                                                                    config.py:262
2024-08-05 15:30:48,979 - anomalib.models.components.base.anomaly_module - INFO - Initializing Draem model.
                    INFO     Initializing Draem model.                                                                                                                                                 anomaly_module.py:42
2024-08-05 15:30:49,503 - anomalib.callbacks - INFO - Loading the callbacks
[08/05/24 15:30:49] INFO     Loading the callbacks                                                                                                                                                           __init__.py:43
2024-08-05 15:30:49,513 - anomalib.engine.engine - INFO - Overriding gradient_clip_val from None with 0 for Draem
                    INFO     Overriding gradient_clip_val from None with 0 for Draem                                                                                                                           engine.py:84
2024-08-05 15:30:49,513 - anomalib.engine.engine - INFO - Overriding num_sanity_val_steps from None with 0 for Draem
                    INFO     Overriding num_sanity_val_steps from None with 0 for Draem                                                                                                                        engine.py:84
2024-08-05 15:30:49,606 - lightning.pytorch.utilities.rank_zero - INFO - Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[08/05/24 15:30:49] INFO     Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default               rank_zero.py:63
                             `ModelSummary` callback.
2024-08-05 15:30:49,614 - lightning.pytorch.utilities.rank_zero - INFO - GPU available: True (cuda), used: True
                    INFO     GPU available: True (cuda), used: True                                                                                                                                         rank_zero.py:63
2024-08-05 15:30:49,615 - lightning.pytorch.utilities.rank_zero - INFO - TPU available: False, using: 0 TPU cores
                    INFO     TPU available: False, using: 0 TPU cores                                                                                                                                       rank_zero.py:63
2024-08-05 15:30:49,616 - lightning.pytorch.utilities.rank_zero - INFO - IPU available: False, using: 0 IPUs
                    INFO     IPU available: False, using: 0 IPUs                                                                                                                                            rank_zero.py:63
2024-08-05 15:30:49,616 - lightning.pytorch.utilities.rank_zero - INFO - HPU available: False, using: 0 HPUs
                    INFO     HPU available: False, using: 0 HPUs                                                                                                                                            rank_zero.py:63
2024-08-05 15:30:49,617 - anomalib.models.components.base.anomaly_module - WARNING - No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use case. Please override `configure_transforms` in your model.
                    WARNING  No implementation of `configure_transforms` was provided in the Lightning model. Using default transforms from the base class. This may not be suitable for your use     anomaly_module.py:235
                             case. Please override `configure_transforms` in your model.
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
2024-08-05 15:30:49,661 - lightning.pytorch.utilities.rank_zero - INFO - You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
                    INFO     You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' |         rank_zero.py:63
                             'high')` which will trade-off precision for performance. For more details, read
                             https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
2024-08-05 15:30:49,686 - anomalib.data.base.datamodule - INFO - No normal test images found. Sampling from training set using a split ratio of 0.20
                    INFO     No normal test images found. Sampling from training set using a split ratio of 0.20                                                                                          datamodule.py:177
2024-08-05 15:30:52,940 - anomalib.metrics.f1_score - WARNING - F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead
[08/05/24 15:30:52] WARNING  F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead                                                f1_score.py:33
2024-08-05 15:30:52,942 - anomalib.metrics.f1_score - WARNING - F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead
                    WARNING  F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead                                                f1_score.py:33
2024-08-05 15:30:53,194 - lightning.pytorch.accelerators.cuda - INFO - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[08/05/24 15:30:53] INFO     LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]                                                                                                                                           cuda.py:58
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                  ┃ Type                     ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ model                 │ DraemModel               │ 97.4 M │
│ 1 │ loss                  │ DraemLoss                │      0 │
│ 2 │ _transform            │ Compose                  │      0 │
│ 3 │ normalization_metrics │ MinMax                   │      0 │
│ 4 │ image_threshold       │ F1AdaptiveThreshold      │      0 │
│ 5 │ pixel_threshold       │ F1AdaptiveThreshold      │      0 │
│ 6 │ image_metrics         │ AnomalibMetricCollection │      0 │
│ 7 │ pixel_metrics         │ AnomalibMetricCollection │      0 │
└───┴───────────────────────┴──────────────────────────┴────────┘
Trainable params: 97.4 M
Non-trainable params: 0
Total params: 97.4 M
Total estimated model params size (MB): 389
/home/jinyao/anaconda3/envs/IADBE/lib/python3.10/site-packages/lightning/pytorch/core/module.py:494: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing
`Trainer(logger=ALogger(...))`
Epoch 999/999 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12/12 0:00:11 • 0:00:00 1.09it/s train_loss_step: 0.222 pixel_AUROC: 0.881 pixel_F1Score: 0.590 train_loss_epoch: 0.2192024-08-05 20:00:55,528 - lightning.pytorch.utilities.rank_zero - INFO - `Trainer.fit` stopped: `max_epochs=1000` reached.
[08/05/24 20:00:55] INFO     `Trainer.fit` stopped: `max_epochs=1000` reached.                                                                                                                                       rank_zero.py:63
Epoch 999/999 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12/12 0:00:11 • 0:00:00 1.09it/s train_loss_step: 0.222 pixel_AUROC: 0.881 pixel_F1Score: 0.590 train_loss_epoch: 0.2192024-08-05 20:00:56,498 - anomalib.callbacks.timer - INFO - Training took 16203.29 seconds
[08/05/24 20:00:56] INFO     Training took 16203.29 seconds                                                                                                                                                              timer.py:59
Epoch 999/999 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12/12 0:00:11 • 0:00:00 1.09it/s train_loss_step: 0.222 pixel_AUROC: 0.878 pixel_F1Score: 0.602 train_loss_epoch: 0.221
2024-08-05 20:00:56,506 - anomalib.metrics.f1_score - WARNING - F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead
                    WARNING  F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead                                                         f1_score.py:33
2024-08-05 20:00:56,507 - anomalib.metrics.f1_score - WARNING - F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead
                    WARNING  F1Score class exists for backwards compatibility. It will be removed in v1.1. Please use BinaryF1Score from torchmetrics instead                                                         f1_score.py:33
2024-08-05 20:00:56,716 - lightning.pytorch.accelerators.cuda - INFO - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[08/05/24 20:00:56] INFO     LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]                                                                                                                                                    cuda.py:58
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2/2 0:00:02 • 0:00:00 0.35it/s  2024-08-05 20:01:01,837 - anomalib.callbacks.timer - INFO - Testing took 4.49368691444397 seconds
Throughput (batch_size=16) : 5.118291602842101 FPS
[08/05/24 20:01:01] INFO     Testing took 4.49368691444397 seconds                                                                                                                                                      timer.py:109
                             Throughput (batch_size=16) : 5.118291602842101 FPS
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│        image_AUROC        │    0.8939393758773804     │
│       image_F1Score       │     0.782608687877655     │
│        pixel_AUROC        │     0.917201817035675     │
│       pixel_F1Score       │    0.5986307859420776     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2/2 0:00:02 • 0:00:00 0.35it/s
